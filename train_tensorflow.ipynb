{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyN/PpIPuc/dwqYfeRDbdnTo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dejunga/ML-Vehicle_Image_Classification/blob/main/train_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'vehicle-classification:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F5521913%2F9142550%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240910%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240910T111846Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D3181f9a75dd97d91817dc2370d400da1917ff2a6dc83d18c683291cf3d15215173ecf33b34150ce0f40378e372215506e4885e339aba2658cddadf87508849acd718ae4569928ec22a1f72bedbef187be2d4675646e4686b612576b864dbfa8998607787999d745ab7465dcfa117d46c194a2482e23c77d2db02d7cfd1b1eee420d8a5364e1782fefbf4c44411d6f3309b376a7fb535ba405d5b233ba7f5903e63476183f6aa51dd3c55f66096a023e0fa139ef3b3e4b382c5929e1501cd9e7209e4459f56c64bf7c500c19f8709cf43451c6e8da3663223eac2d9301b819e2665f6b9c62ff51fc68ba753059235f999a3defcabf57aa73533452950d7e7c506'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t25ffxuQymVf",
        "outputId": "86f2371a-e934-447d-98f8-5d9d5d432a8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading vehicle-classification, 866783573 bytes compressed\n",
            "[==================================================] 866783573 bytes downloaded\n",
            "Downloaded and uncompressed: vehicle-classification\n",
            "Data source import complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8t8-5sARvfnu",
        "outputId": "9be362d3-64ad-4f26-f82b-ab138fc0fefe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Found 560 images in train/Auto Rickshaws\n",
            "Found 560 images in train/Bikes\n",
            "Found 553 images in train/Cars\n",
            "Found 560 images in train/Motorcycles\n",
            "Found 560 images in train/Planes\n",
            "Found 560 images in train/Ships\n",
            "Found 560 images in train/Trains\n",
            "Found 120 images in val/Auto Rickshaws\n",
            "Found 120 images in val/Bikes\n",
            "Found 118 images in val/Cars\n",
            "Found 120 images in val/Motorcycles\n",
            "Found 120 images in val/Planes\n",
            "Found 120 images in val/Ships\n",
            "Found 120 images in val/Trains\n",
            "Found 120 images in test/Auto Rickshaws\n",
            "Found 120 images in test/Bikes\n",
            "Found 119 images in test/Cars\n",
            "Found 120 images in test/Motorcycles\n",
            "Found 120 images in test/Planes\n",
            "Found 120 images in test/Ships\n",
            "Found 120 images in test/Trains\n",
            "Found 3911 images belonging to 7 classes.\n",
            "Found 838 images belonging to 7 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            " 44/123 [=========>....................] - ETA: 10:00 - loss: 1.3198 - accuracy: 0.5987"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "123/123 [==============================] - 1234s 10s/step - loss: 0.8533 - accuracy: 0.7228 - val_loss: 0.3384 - val_accuracy: 0.8819\n",
            "Epoch 2/10\n",
            "123/123 [==============================] - 81s 663ms/step - loss: 0.4953 - accuracy: 0.8284 - val_loss: 0.2542 - val_accuracy: 0.9129\n",
            "Epoch 3/10\n",
            "123/123 [==============================] - 81s 657ms/step - loss: 0.4396 - accuracy: 0.8387 - val_loss: 0.2539 - val_accuracy: 0.9117\n",
            "Epoch 4/10\n",
            "123/123 [==============================] - 82s 669ms/step - loss: 0.4210 - accuracy: 0.8486 - val_loss: 0.2502 - val_accuracy: 0.9117\n",
            "Epoch 5/10\n",
            "123/123 [==============================] - 82s 664ms/step - loss: 0.3755 - accuracy: 0.8622 - val_loss: 0.2492 - val_accuracy: 0.9165\n",
            "Epoch 6/10\n",
            "123/123 [==============================] - 82s 663ms/step - loss: 0.3833 - accuracy: 0.8614 - val_loss: 0.2376 - val_accuracy: 0.9212\n",
            "Epoch 7/10\n",
            "123/123 [==============================] - 82s 666ms/step - loss: 0.3734 - accuracy: 0.8783 - val_loss: 0.2363 - val_accuracy: 0.9189\n",
            "Epoch 8/10\n",
            "123/123 [==============================] - 82s 663ms/step - loss: 0.3483 - accuracy: 0.8775 - val_loss: 0.2183 - val_accuracy: 0.9260\n",
            "Epoch 9/10\n",
            "123/123 [==============================] - 81s 658ms/step - loss: 0.3353 - accuracy: 0.8785 - val_loss: 0.2186 - val_accuracy: 0.9284\n",
            "Epoch 10/10\n",
            "123/123 [==============================] - 81s 663ms/step - loss: 0.3093 - accuracy: 0.8883 - val_loss: 0.2341 - val_accuracy: 0.9236\n",
            "Training history saved at /content/drive/My Drive/models/model_history.pkl\n",
            "Model trained and saved to /content/drive/My Drive/models/tensorflow_vehicle_model.keras successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "import pickle  # Added to save the history\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set up directories for training data\n",
        "main_dir = \"/content/drive/My Drive/Vehicles\"  # Adjust this to your path\n",
        "categories = [\"Auto Rickshaws\", \"Bikes\", \"Cars\", \"Motorcycles\", \"Planes\", \"Ships\", \"Trains\"]\n",
        "base_split_dir = \"/content/drive/My Drive/vehicles_split\"\n",
        "\n",
        "# Check if the base_split_dir exists, if not create it and split the dataset\n",
        "if not os.path.exists(base_split_dir):\n",
        "    split_dirs = [\"train\", \"val\", \"test\"]\n",
        "\n",
        "    for split_dir in split_dirs:\n",
        "        for category in categories:\n",
        "            os.makedirs(os.path.join(base_split_dir, split_dir, category), exist_ok=True)\n",
        "            print(f\"Directory created: {os.path.join(base_split_dir, split_dir, category)}\")\n",
        "\n",
        "    # Split the data and move images to appropriate directories\n",
        "    for category in categories:\n",
        "        category_path = os.path.join(main_dir, category)\n",
        "\n",
        "        # Ensure the directory exists\n",
        "        if not os.path.exists(category_path):\n",
        "            print(f\"Directory not found: {category_path}\")\n",
        "            continue\n",
        "\n",
        "        images = os.listdir(category_path)\n",
        "        train_images, temp_images = train_test_split(images, test_size=0.3, random_state=42)\n",
        "        val_images, test_images = train_test_split(temp_images, test_size=0.5, random_state=42)\n",
        "\n",
        "        # Function to move images into respective folders\n",
        "        def move_images(image_list, destination):\n",
        "            for image in image_list:\n",
        "                shutil.copy(os.path.join(category_path, image), os.path.join(base_split_dir, destination, category, image))\n",
        "\n",
        "        move_images(train_images, \"train\")\n",
        "        move_images(val_images, \"val\")\n",
        "        move_images(test_images, \"test\")\n",
        "\n",
        "    print(\"Data split into train, validation, and test sets successfully!\")\n",
        "\n",
        "# Check how many images were copied into each category and split\n",
        "split_dirs = [\"train\", \"val\", \"test\"]\n",
        "for split_dir in split_dirs:\n",
        "    for category in categories:\n",
        "        num_images = len(os.listdir(os.path.join(base_split_dir, split_dir, category)))\n",
        "        print(f\"Found {num_images} images in {split_dir}/{category}\")\n",
        "\n",
        "# Create data generators\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20, width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Ensure the directories exist before creating generators\n",
        "if os.path.exists(os.path.join(base_split_dir, \"train\")):\n",
        "    train_generator = train_datagen.flow_from_directory(os.path.join(base_split_dir, \"train\"), target_size=(150, 150),\n",
        "                                                        batch_size=32, class_mode='categorical')\n",
        "else:\n",
        "    print(f\"Training directory not found: {os.path.join(base_split_dir, 'train')}\")\n",
        "\n",
        "if os.path.exists(os.path.join(base_split_dir, \"val\")):\n",
        "    val_generator = val_test_datagen.flow_from_directory(os.path.join(base_split_dir, \"val\"), target_size=(150, 150),\n",
        "                                                         batch_size=32, class_mode='categorical')\n",
        "else:\n",
        "    print(f\"Validation directory not found: {os.path.join(base_split_dir, 'val')}\")\n",
        "\n",
        "# Load VGG16 as the base model\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
        "base_model.trainable = False\n",
        "\n",
        "# Build the model\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(categories), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# ***MODIFICATION STARTS HERE***\n",
        "# Train the model and store the training history\n",
        "history = None\n",
        "if os.path.exists(os.path.join(base_split_dir, \"train\")) and os.path.exists(os.path.join(base_split_dir, \"val\")):\n",
        "    history = model.fit(train_generator, epochs=10, validation_data=val_generator)\n",
        "\n",
        "# Save the training history\n",
        "history_save_path = '/content/drive/My Drive/models/model_history.pkl'\n",
        "with open(history_save_path, 'wb') as file_pi:\n",
        "    pickle.dump(history.history, file_pi)\n",
        "\n",
        "print(f\"Training history saved at {history_save_path}\")\n",
        "# ***MODIFICATION ENDS HERE***\n",
        "\n",
        "# Save the model in .keras format\n",
        "save_dir = '/content/drive/My Drive/models'\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "model_save_path = os.path.join(save_dir, 'tensorflow_vehicle_model.keras')\n",
        "model.save(model_save_path)\n",
        "\n",
        "print(f\"Model trained and saved to {model_save_path} successfully!\")"
      ]
    }
  ]
}